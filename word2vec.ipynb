{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Artefacts\n",
    "\n",
    "The notebook shows how to produce a model and artefacts for a model that generates predictions of embedding components and the result is computed as the minimum distance to elements of an embedding dataset with a configurable distance function. The artefacts produced are:\n",
    "* tensorflow saved model\n",
    "* embedding dataset\n",
    "* verification samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Word2Vec Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k', \n",
    "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "    with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_data.shuffle(1000).padded_batch(10)\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "embedding_dim=16\n",
    "\n",
    "model = K.Sequential([\n",
    "  K.layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  K.layers.GlobalAveragePooling1D(),\n",
    "  K.layers.Dense(16, activation='relu'),\n",
    "  K.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches, validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a model that predicts the embedding components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = K.models.clone_model(model)\n",
    "prediction_model.pop()\n",
    "prediction_model.pop()\n",
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract embedding weights from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.layers[0].get_weights()[0][1:,:]\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate verification data from model and embeddings\n",
    "\n",
    "In this example we compute the cosine distance for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_input = test_batches.unbatch().batch(1).take(10)\n",
    "num_results = 5\n",
    "requests = [{\n",
    "    'input': [[int(x) for x in e[0][0]]],\n",
    "    'num': num_results,\n",
    "    'distance': 'cosine'\n",
    "} for e in list(verification_input.as_numpy_iterator())]\n",
    "\n",
    "prediction_output = prediction_model.predict(verification_input)\n",
    "\n",
    "def norm(m):\n",
    "    return m / np.sqrt(np.sum(m * m, axis=-1, keepdims=1))\n",
    "\n",
    "scores = norm(prediction_output) @ norm(embedding_weights).T\n",
    "\n",
    "examples = prediction_output.shape[0]\n",
    "scored_ix = np.arange(examples).reshape(-1, 1)\n",
    "top_k = scores.argpartition(-num_results)[:,-num_results:]\n",
    "sorted_k = top_k[scored_ix, (scores[scored_ix, top_k]).argsort()]\n",
    "scores_k = scores[scored_ix, sorted_k]\n",
    "\n",
    "responses = [\n",
    "    {'result': [{'term': encoder.decode([i + 1]).rstrip(), 'score': float(s)}\n",
    "                for i, s in zip(terms, scores)]}\n",
    "    for terms, scores in zip(top_k, scores_k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the layers of the created model\n",
    "\n",
    "In order for our service to figure out which parameter passed into our predict api should go to the model, we need to make sure the model's input layer is named. Below, we use an `InputLayer` with the name `tokens`, so we can expect the api to look something like `/api/predict?tokens=[[111, 222, 333]]`. Later in the workbook we explain how to verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = K.Sequential([\n",
    "    K.layers.InputLayer(input_shape=(None,), name='tokens'),\n",
    "    prediction_model,\n",
    "    K.layers.Lambda(lambda x: tf.reduce_mean(x, axis=0), name='result')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out all Artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/word2vec/model\n",
    "saved_model_dir = '/tmp/word2vec/model'\n",
    "model_to_save.save(saved_model_dir)\n",
    "\n",
    "!tar -cvzf /tmp/word2vec/model.tgz -C /tmp/word2vec/model .\n",
    "\n",
    "pd.DataFrame(\n",
    "    embedding_weights,\n",
    "    index=pd.Index(\n",
    "        [encoder.decode([i]).rstrip() for i in range(1, encoder.vocab_size)],\n",
    "        name='term')\n",
    ").to_csv('/tmp/word2vec/embedding.csv')\n",
    "\n",
    "with open('/tmp/word2vec/verification.jsonl', 'wt') as f:\n",
    "    for req, resp in zip(requests, responses):\n",
    "        json.dump({'request': req, 'response': resp}, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "!ls -l /tmp/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models may require the definition of keras layers performing custom transformations. A good check is to re-load the model from disk as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_disk = tf.keras.models.load_model(saved_model_dir)\n",
    "model_from_disk.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll double-check the input tensor and it's name. There's a little bit of data cleaning because of how Tensorflow stores its input signatures, but from the printed output, we can see that the input tensor has the name of `tokens`, which is exactly what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input Tensors: ', [tensor for tensor in model_from_disk.signatures['serving_default'].structured_input_signature if tensor]) # Cleanup empty inputs\n",
    "print('Output Tensors: ', model_from_disk.signatures['serving_default'].structured_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
